{
  "title": "Announcing Manival",
  "date": "June 18, 2025",
  "subtitle": "How Manival works",
  "category": "artificial intelligence",
  "preview_image": "https://substackcdn.com/image/fetch/$s_!-A0s!,w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc171ea3e-515e-473d-8490-ac0e1dea13ee_2819x1644.png",
  "content": "<p><em>Status: Something we’ve hacked on for a couple of weeks; looking to get feedback and iterate!</em></p>\n<p>Last time, I wrote about some <a href=\"https://manifund.substack.com/p/givewell-for-ai-safety-lessons-learned\">considerations</a> for AI safety grant evaluation, but didn’t actually ship a cost-effectiveness model. Since then, Austin, Nishad, and I have:</p>\n<ul>\n<li>\n<p>Developed <a href=\"https://manivaluator.org/\">Manival</a>, an LLM-powered grant evaluator</p>\n</li>\n<li>\n<p>Demoed it to an audience at <a href=\"https://manifest.is/\">Manifest</a></p>\n</li>\n<li>\n<p>Written and applied <a href=\"https://docs.google.com/spreadsheets/d/1t4GkdnurnDAb8N_tO5Kl6RuEY83Nll7hA0FscezqGFU/edit?usp=sharing\">our own grantmaking criteria</a>—we’ll see if Manival can replicate our taste </p>\n</li>\n</ul>\n<p><img alt=\"\" src=\"images/announcing-manival_img_01.png\" /></p>\n<p>This is effectively a form of structured ‘Deep Research’.</p>\n<p>First, we specify the fields that matter to us when evaluating a grant. These might include ‘domain expertise of project leads’ or ‘strength of project’s theory of change’. We have RAG-based ‘data fetchers’ (Perplexity Sonar) scour the internet and return a score, with reasoning, for each of these fields. We then feed these into an LLM synthesizer (Claude Opus), which provides an overall evaluation.</p>\n<p>This is a pretty janky LLM wrapper compensating for the lack of Deep Research API. We’re aware of various RAG and Deep Research alternatives, and expect our evaluations to improve as we plug better models in.</p>\n<p><img alt=\"\" src=\"images/announcing-manival_img_02.png\" /></p>\n<h3>Customizing the criteria</h3>\n<p>Different people have different ideas of what should go into a grant evaluation config. Austin cares deeply about how great a team is; I’d like mine to consider counterfactual uses of a team’s time.</p>\n<p>With Manival, you can apply any grant evaluation criteria of your choosing (go to Configs → AI Generate). Here’s one we made just for fun:</p>\n<p><img alt=\"\" src=\"images/announcing-manival_img_03.png\" /></p>\n<p><img alt=\"\" src=\"images/announcing-manival_img_04.png\" /></p>\n<h3>What’s next for Manival?</h3>\n<p>Manival has lots of potential uses. Here are some main ones:</p>\n<ol>\n<li>\n<p><strong>Estimating marginal cost-effectiveness:</strong> We could write a config that estimates how much of a difference marginal $ <x> would make.</p>\n</li>\n<li>\n<p><strong>Predicting impact market cap:</strong> Right now, our configs evaluate projects on a scale from 0 to 10. In the real world, project size varies: some established projects seek 6-7 figures like a ‘Series A’; others seek 4-5 figures in ‘seed’ / ‘pre-seed’ funding. Can we use Scott Alexander’s <a href=\"https://forum.effectivealtruism.org/posts/E7pkeDruknpSa7j3i/results-of-an-informal-survey-on-ai-grantmaking\">impact valuations</a> to estimate a project’s ‘impact market cap’?</p>\n</li>\n<li>\n<p><strong>Improving project proposals:</strong> Grant applicants can run their project proposal through Manival to understand what might need clarifying.</p>\n</li>\n<li>\n<p><strong>Project comparison:</strong> We can use Manival to rank a category on Manifund, funnelling its most underrated projects to the top of your feed.</p>\n</li>\n<li>\n<p><strong>Recommendations:</strong> We can use Manival to recommend new projects to grantmakers based on projects they’ve already supported.</p>\n</li>\n<li>\n<p><strong>Solving the ‘adverse selection’ / ‘funging’ problems:</strong> Grantmakers can estimate how likely a project might be to get funding elsewhere, or better understand why it hasn’t been funded when that’s the case.</p>\n</li>\n</ol>\n<p>It might be valuable to simulate how other grantmakers you respect might evaluate a project when deciding whether to make a grant. For example, here’s a simulation of Joe Carlsmith’s thinking:</p>\n<p><img alt=\"\" src=\"images/announcing-manival_img_05.png\" /></p>\n<p>These ‘simulated scores’ might differ from how a grantmaker actually thinks. Accordingly, we plan to develop configs that are maximally faithful to <a href=\"https://docs.google.com/spreadsheets/d/1t4GkdnurnDAb8N_tO5Kl6RuEY83Nll7hA0FscezqGFU/edit?usp=sharing\">our own thinking</a> over the next week.</p>\n<p>For now, I expect a lot of Manival’s value to come from ‘flagging potentially great projects to look into’, rather than being something people defer to.</p>\n<p>We’re excited for you to try Manival, and eager to know what you think, especially if you’re a donor, grantmaker, or someone else who cares a lot about evaluating grant proposals. <a href=\"https://calendly.com/manival/\">Schedule a call</a> with us to chat this through, or let us know in the comments!</p>\n<hr />\n<p><em>Originally published on <a href=\"https://manifund.substack.com/p/announcing-manival\">The Fox Says</a></em></p>"
}