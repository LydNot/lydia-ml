{
  "title": "Update re 'spontaneous instantiation'",
  "date": "April 07, 2025",
  "subtitle": null,
  "category": "technology & society",
  "content": "<p><a href=\"javascript:void(0)\">Share</a></p>\n<p>I recently wrote a story about how scheming / deceptive alignment might arise. I basically drew from Kokotajlo’s story¹ that a model might de-emphasize concepts that interfere with effectiveness, trade off one Spec trait (e.g. “honest”) for another (e.g. “helpful”), learn instrumental goals it comes to treat as terminal, and so on.</p>\n<p>My affinity for Kokotajlo’s story stemmed from not believing in ‘spontaneous instantiation’ — that is, believing that everything has a cause; ‘it’s not possible that something can come from nothing’. I didn’t understand how a model could spontaneously instantiate goals that are anti-human, unless it’s in response to some aspect of its programming or training, inputs we provide it.</p>\n<p>Y pointed out that this doesn’t necessarily apply so strongly in the case of AI models. I should’ve remembered this from the induction heads paper²: sometimes ‘grokking’ occurs very rapidly.³ These ‘phase transitions’ can look a lot like something coming from nothing (although we may be able to reverse-engineer what happened and why, such that the causality seems obvious in retrospect). I should expect to see more apparent ‘spontaneous instantiation’ going forward.</p>\n<p>¹ <em><a href=\"https://ai-2027.com/research/ai-goals-forecast\">Forecasting AI Goals</a></em> , Kokotajlo</p>\n<p>² <em><a href=\"https://arxiv.org/abs/2209.11895\">In-context Learning and Induction Heads</a></em> , Olsson et al.</p>\n<p>³ <em><a href=\"https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/\">Future ML Systems Will Be Qualitatively Different</a></em> , Steinhardt</p>\n<p><a href=\"javascript:void(0)\">Share</a></p>\n<hr />\n<p><em>Originally published on <a href=\"https://lydianottingham.substack.com/p/update-re-spontaneous-instantiation\">Substack</a></em></p>"
}